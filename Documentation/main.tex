\documentclass[
11pt,
twoside
]{report}


\usepackage[
a4paper,
vmargin=1in,
hmargin=1in
]{geometry}

\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{multirow}


\usepackage{graphicx}
\graphicspath{{resources/images/}}



\usepackage[ backend=biber ]{biblatex}
\addbibresource{./resources/references.bib}



\usepackage{algpseudocode}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input: }}
\renewcommand{\algorithmicensure}{\textbf{Output: }}
\algnewcommand\And{\textbf{ and }}
\algnewcommand\Or{\textbf{ or }}
\algnewcommand\Is {\textbf{ is }}
\algnewcommand\Not {\textbf{ not }}



\usepackage{amssymb}



\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}


\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[C]{\ThesisTitle}
\fancyfoot{}
\fancyfoot[C]{\thepage}



\newcommand{\ThesisTitle}{Computational Imaging}
\newcommand{\TheAuthor}{Jose Fernandes}
\title{
{\ThesisTitle}\\
\hfill \\
{\large Heriot-Watt University}\\
\hfill \\
{\includegraphics[scale=2.5]{hw.jpg}} \\
\hfill \\
}
\author{\TheAuthor}
\date{22 November 2021}


\doublespacing
\begin{document}

\maketitle

\chapter*{Declaration}

I, \TheAuthor, confirm that this work submitted for assessment is my own and is expressed in my own words. Any uses made within it of the results of other authors in any form (e.g., ideas, equations, figures, text, tables, programs) are adequately acknowledged at any point of their use. A list of the references employed is included.

\hfill


Signed: \TheAuthor


Date: 22/11/2021

\chapter*{Acknowledgement}


Acknowledgements go here






\chapter*{Abstract}

This Project describes creating a multi-stage algorithm to track and extract features from football matches' 2D video recording. The solution will track and map the players and ball's position relative to the court lines with minimal human assistance or monitoring. This algorithm will require implementing image processing and machine learning tools and techniques to allow visual recognition of intended elements. Although there are significant technical challenges, similar solutions were already attempted, which lay down the groundwork and references for guidance in this Project.


This solution aims to be an open platform for computer and sports scientists to collaborate and make invaluable contributions to the advancement of sports data science.




\tableofcontents


\chapter{Introduction}

\section{Motivation}


In recent years, the importance of data analytics in sports allied with domain expertise has become ever more essential. A recent great example is the revival of one of the best football clubs globally, the Liverpool Football Club (LFC). The LFC was initially criticised for their analytical approach to running the football team, but now it is regarded as one of the most successful and profitable organisations in the sports industry. This organisation adopted innovative management strategies guided by advanced match analytics analysis to measure their performance and calculate the likelihood of their action's success (and profit) \cite{liverpool}.


Despite this encouraging story \cite{liverpool}, and modern technology's availability, the advanced analytics data is only within reach of high-income clubs and organisations due to the costs involved \cite{opta}. The data providers rely primarily on human agents to label and annotate the actions and events in a match which drive the cost up \cite{opta}. These datasets are therefore not freely available for any small organisation, researcher or enthusiast that could contribute, for example, to various areas of Artificial Intelligence (AI), including Game Theory \cite{deepmind}.


The proposed solution is an free- and open-source computer vision framework that extracts features from 2D video single-camera broadcasting, using image processing and machine learning tools, to output the position of players and ball sequentially. These principles can be applied to other sports, and the data produced will be vital to the sports data science community because it will encourage innovation in the sports field. This software will adopt an open-source license to encourage redistribution and collaboration between interested individuals and organisations, which will promote free access of football tracking data to anyone interested \cite{osd}.  This report will cover the implementation of this solution.


It will cover an extensive explanation of  discussion about similar research material and projects related, comprehensive descriptions about its implementation, testing and respective conclusion.



\section{Main Objectives}



This project is highly ambitious so the ultimate goal is to create a solid foundation to, on the long term, allow other solutions to be built on its top. The solution must be highly modular and effectively aim to act more as a library than a framework, composed of smaller, reusable and generic components. Different collective sports have different dynamics and challenges so giving other sports' developers room to apply their sports flow upon the solution is very important. The advantage behind this is to create an environment where the different smaller modules used can the more robust as the variety of target sports increases. If the objective of having an highly varied set of sports' enthusiasts is met, then the main branch of development can focus on the canonical limitations. The non-functional requirements of just focusing on a static 2D camera match stream is to avoid adding more complexity to the problem and to not leave some sports out due to singular exceptions regarding the way a particular sport is recorded.


Due to the problem high complexity the program must be a semi-automated solution first, at least at this stage. Uncountable issues arise from processing a unpredictable real-world event so it is impossible to write a program that handles all cases. As the program becomes ever more robust, we could let it run independently based on an high rate of confidence on handling most frequent/critical types of event. This may be out of this stage's scope. The strategy must then rely on giving the operator control over important functions and then develop mechanisms that will increment their efficiency over time reducing gradually the activity of an human. The value proposition is ultimately to first reduce the number of operators and then reducing the need for one.


Object recognition models are very prevalent in the industry and accessible. However when creating a computing imaging solution for sports it is imperative to create a machine learning model that can essentially map objects from pitch to the referential pitch's map. This may be difficult to do manually since the dataset with labelled pitches are scarce if not unavailable for most sports. It is crucial to develop a strategy that can overcome this scarcity without compromising its generality. The Microsoft's ``fake it until you make it'' project \cite{ms_fake} gives an hint how such strategy may resemble. It used a synthetic dataset to overcome a scarcity problem. The objective, in this project, is to create a module that concerns creating a synthetic dataset for each sport.


The option for a free- and open-source library is, as mentioned before, a priority. It promotes unrestricted collaboration which is fundamental for this endeavour. Despite this obvious advantage, there are some libraries that have non-compatible licenses, even if open-source. Any library used must be compatible with the chosen license otherwise the same result must be accomplished by implementing the solution from scratch.



\section{Issues Resolved}


Processing the image is the first step when trying to extract data from the video stream. The OpenCV library \cite{opencv} is very useful for this matter since it has all the utilities necessary to manipulate images and video streams. Many strategies were attempted such as using the library to recognise objects or poses estimation using OpenCV pre-trained models but they were not successful because of the scale the players and ball on the camera screen. Detecting objects is only the first step in extracting data, tracking objects over the screen is critical and it was not easy to implement and integrate into the detection system. The main issue was that tracked players, due to change in the camera angle, would move away from the camera screen which would then disabled their tracker. The algorithm was then modified to perform the detection every 30 seconds or whenever a tracker loses sight of its tracker. In this was we accomplish a reasonable result that will be the foundation for further development and improvement of detection and tracking. Additionally, the library was very effective in reading the video stream to the machine learning algorithms because it had to, for example, remove noise, resize and encode images before piping it to a model input. These operations are very important not only to provide the input (machine learning models have strict interfaces) to the models but also to improve their performance.


As mentioned before, the OpenCV object detection pre-trained models were not successful in achieving a decent result. The YOLO convolution network model (based on PyTorch) was implemented to detect humans and the ball on the screen. Its setup had some issues mostly because the pre-trained weights and the network configuration is specific to the version 3 which has a different implementation from previous ones. Up to this moment, all libraries used were compatible with the desired General Public License version 3 (GPL3) so some functionalities applied with Numpy \cite{numpy} (BSD license, not compatible with GPL3) were substituted by hard-coded ones which took extra time. The final results worked really well as players were detected consistently but the not so much the ball. The players detection had a initial issue of producing redundant and overlapping detection for the player. This was fixed by applying the non-maximum suppression algorithm given the people bounding boxes to then filtering out those predictions.



The blender 3d model was a big task because it was meant to recreate a realistic football pitch that could be photographed and extracted data from. This model followed the official measurements and included the grass, the lines, the goals and, corner flags and, most important, the 3d model markers. These objects were built from scratch following online resources and took a while to refine them up to a decent level. Following that was the creation of a referential map upon which the dataset generating algorithm would map the 3d model markers from the their Cartesian coordinates to the camera screen 2d coordinates. Once these tasks were finished, the next step was to automate the geometric and imaging data using Blender's Python API. This was at times challenging because the API is quiet advanced and sophisticated so it has a steep learning curve associated with it. One issue before completion of automating the model, was the rendering option which would return a negative colour pallet that would not be appropriate to train with. This issue was solved by tweaking the rendering option until it returned the correct image output.



A new machine learning model had to be developed to train, test and apply it using the synthetic dataset. The Tensorflow machine learning framework was chosen because it has Apache version 2 license which is compatible with GPL3. Unfortunately this framework requires the Numpy \cite{numpy} library to input data. Many where the attempts to circunvent the latter library but with no success. The Numpy \cite{numpy} library was implemented what caused a change in the target license in detriment of the Lesser General Public License version 3 (LGPL3). The model was more complex than usual and involved constant tinkering in the encoding/decoding of data (to determine which is the most effective data structure layout) and the machine learning model architecture/configuration.




\section{Methods}


The first and most important method is to work on in the image and video processing operations. These techniques are crucial for the image recognition technologies because they read the video stream as a sequence on images which are processed before the detection and tracking operations. The reading of the stream is in charge of the OpenCV library by running a while loop that reads the image sequence from the video capture object.


The pitch recognition required some extra image processing that concerned applying a colour conversion, resizing and a bitwise ``and'' operation to return a ``green'' masking to remove the image noise from the outside of the pitch.


The tracking is the domain of the OpenCV library by running a ``MultiTracker'' which aggregates a set of trackers for human tracking and a single one for the ball one. The type of tracker use is called ``CSRT'' based on the paper ``Discriminative Correlation Filter with Channel and Spatial Reliability'' \cite{csrt}. This type of tracker has a slower execution time but has the best result of all other available trackers on the computer vision library, even when the camera moves drastically.


The OpenCV library has also some manual controls that are very useful for the video stream manual operation and to aid ball tracking. It has some event handlers for key presses that are used to quit the streaming (by pressing the ``q'' key) and to trigger the manual ball labelling. The ball labelling is a GUI control that is triggered by the ``f'' key press.


The human and ball recognition is undertaken using the YOLO version 3, as mentioned before, that is already widely used in similar tasks. It is a pre-trained and -configured convolutional network that detects many a extensive range of types of objects that are contained in the ``coco.names'' helper file which comes along with the ``yolov3.cfg'' (for configuration) and ``yolov3.weights'' (for neurons' weights). The algorithm used in this project filters out all detections except the humans and the ball and applies another filtering to humans and ball detections (the ones below their threshold). The OpenCV's ``Non-maximum suppression'' \cite{nms} function is ran since, even after the filtering of predictions, the model was outputting many redundant and overlapping detections. This last function manages to reduce most redundancy and still capture some players even when they crossing (can be calibrated with a threshold value).


The synthetic image dataset production started out by creating a 2d png image file of a football pitch (transparent with white lines, done with GNU image manipulator program (GIMP)). This file was then merged into a 2d plane texture compositor to recreate the look of a football match. Some objects where also created using the blender's tools to add some more realism to model. Apart from 3d modelling, the blender file was ran through a python script that automates the movement, data collection and rendering of images of a set of cameras. The image data is stored into individual files and the geometric data is written to a single file


The resultant data must then be decoded and manipulated, running the Numpy library, into the deep learning convolutional network. The recursiveness of this model forces the pipeline to be very flexible to handle single and multiple inputs (with varying input shapes). The recursive prediction is also conditional operation because it uses the current prediction to feed it (along the same image) to the next recursive prediction



\section{Results}

results are below

(sshot\textgreater{} image recognition, ball and humans)

\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{first.png}
    \caption{First detection, no ball detected}
    \label{img:1}
\end{figure}
\ref{img:1}
players are detected but ball is not (purple means that object recognition just ran). multiple players are detected in the same bounding box. refs are detected as well. one steward is also detected. streaming is stopped because ball is not found.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_21-32-51.png}
    \caption{Manually labelling ball}
    \label{img:2}
\end{figure}
\ref{img:2} players bounding boxes in green means that it is tracking. manually labelling ball to continue stream.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_21-35-39.png}
    \caption{Ball tracking error}
    \label{img:3}
\end{figure}
\ref{img:3} ball tracking is lost and tracks the numbers on the players back.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_21-36-05.png}
    \caption{New players on screen are not detected}
    \label{img:4}
\end{figure}
\ref{img:4} player tracking continues. new players appear on the screen but they are not
detected until 30frame period runs object detection again.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_21-38-04.png}
    \caption{Ball tracking lost}
    \label{img:5}
\end{figure}
\ref{img:5} ball
tracking is lost again because of the pitch lines and player boots. some players previously detected are lost because of the backgroup from ads or pitch (not enough constract).
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_21-36-47.png}
    \caption{New ball labelling}
    \label{img:6}
\end{figure}
\ref{img:6} ball needs
to be labelled again to be tracked again.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-05-49.png}
    \caption{Ball tracking running, some players go off screen and tracking lost}
    \label{img:7}
\end{figure}
\ref{img:7} the object
detection is ran. all the human trackers are removed but the ball tracker. the ball tracker is not reset if the ball is still being tracked.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-06-03.png}
    \caption{Player's tracking is lost again, ball tracking continues}
    \label{img:8}
\end{figure}
\ref{img:8} the
players tracker had to be reset again from the because the detected players moved out of the screen in the meantime. ball tracker continues to run, regardless.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-06-18.png}
    \caption{Players is running but some players are not detected, ball tracking is wrong}
    \label{img:9}
\end{figure}
\ref{img:9} the ball
tracker is wrong again, by tracking the player's back number.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_21-39-53.png}
    \caption{Different camera angle}
    \label{img:10}
\end{figure}
\ref{img:10} this is a
video segment imposed by the director. data cannot be collected. this is a short moment.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-06-43.png}
    \caption{One wrong human detection and ball detection is wrong as well}
    \label{img:11}
\end{figure}
\ref{img:11} ball is
tracked from the since the label needs to be labelled for the stream to continue (needs to cut the scene). there is a bug, a human is recognised due to noise.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-08-00.png}
    \caption{Detection bug continues, fan is detected and ball tracking is lost}
    \label{img:12}
\end{figure}
\ref{img:12} players
are detected again. the bug still persists. a fan is recognised in the crowd. the ball is again not recognised.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-09-45.png}
    \caption{New perspective, players were detected and ball is being labelled}
    \label{img:13}
\end{figure}
\ref{img:13} most
players are visible from this new perspective. The ball is not recognised.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-10-58.png}
    \caption{New perspective, players and ball are being tracked}
    \label{img:14}
\end{figure}
\ref{img:14} most of
the players are recognised from this perspective. The ball is visible and being tracked.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-12-29.png}
    \caption{Yet another perspective, most players detected but ball tracking is wrong}
    \label{img:15}
\end{figure}
\ref{img:15} in this
frame few players were recognised because the camera view moved very suddendly (from the previous frame ) in betwee the object detection cycle. The ball tracker was lost and it is tracking the payer's leg instead.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-03_23-13-18.png}
    \caption{Half of players are detected, all in the most important zone}
    \label{img:16}
\end{figure}
\ref{img:16} In this
frame most players are detected including all players inside the box which is the region of interest when a team is attacking. The ball tracker has lost the ball because the camera view is being blocked by the crossing player's leg. (sshot\textgreater{} pitch 3d modelling and camera automation)





\section{Achievements and Limits}

\subsection{Achievements}


The most important achievement is the development of the foundations for a generic sports video data collection framework, as explained before. This was a challenging task since there are any other public full featured solution as this attempts to be. Although not all main features are not yet finished the project was planned to be a long term endeavour.


The image object detection and tracking algorithm is the most critical component in this system because without it data collection is not possible. This module may be changed but the algorithm will, a priori, remain the same as it fits the needs of other collective sports that are played with a ball.


The use of synthetic data is a novel and innovative way to approach the problem of creating a dataset for pitch recognition because it offers new and interesting possibilities when creating computer vision projects for sports. The current model can be extended to include other sports by, for example, add more scenes for different sports models.


Another relevant achievement is the production of an end-to-end dataset generation framework that automates a set of cameras to simulate their normal activity. This framework is now mature and can already, as all the other modules, accommodate for other sports which is invaluable because it is now possible to create huge datasets for any sport.


The custom machine learning engine is an highlight of the system because it executes a recursive conditional model. This engine has shown great potential because it is a neat solution for a complex problem of executing a geometric reconstruction of a match recording scene. By recursively and conditionally executing these models it is possible to have better results than a single model.




\subsection{Limits}


This project has many limits at this stage of development. As mentioned before, there is still a need for a human agent to verify/validate the data collection, manually segment the video segments supervise/calibrate video processing and deal with some broadcasting events such as zoom, replays and camera changes. These are instances that need a deeper level of reasoning which is not available in the current system. It is expected, as the system increases its performance and robustness, to adding functionalities that handle this events but they are expected to be tackled in a later stage of development as the cost effectiveness ratio of their implementations are more appealing.


The object detection is not perfect at this moment either. Humans are detected consistently but sometimes the same detection may be catching two players instead of doing two individual detections. This limitation must be mitigated because it may cause noise to the out put data. The ball detection is more problematic because it is not consistent and forces the operator to frequently manually label the ball. The manual labelling of the ball is not efficient in terms of time so it is a major priority to improve the ball detection success rate before the system's production or release.



\subsection{Dissertation organisation sketch}


to fill in


\chapter{Background}


This chapter will delve into current research, projects and articles related to computer vision for sports' data extraction. The interest in this field is recent. However, there is already a comprehensive set of information sources necessary to evaluate the feasibility of developing a potential solution. Most of these sources lay out the scientific and technical principles applied in developing their successfully tested and solutions being assessed. Therefore, it is vital to have a deeper analysis and reflection on these principles to determine the most appropriate approach for this Project. The review aims to gather the knowledge required to develop and evaluate this Project's solution successfully.


\section{Basketball video analysis}


The ``Open Source Sports Video Analysis using Machine Learning'' article \cite{stephan} is an excellent article that lays out a real-world application of the theory behind automated sports analytics video extraction. It serves as a complete description of the ``basketballVideoAnalysis'' open-source project \cite{stephan_code} that has the goal of tracking players, objects and actions for allowing the analysis of basketball match recordings. This article is a good starting point for this review because Stephan does a good job explaining complex theories in more straightforward terms and referencing other authoritative resources that must be considered to further expand on if anyone is interested in developing a similar solution. The Project used the ``Player tracking and analysis of basketball plays'' \cite{baskettrack} research as the blueprint for its planning and development. Stephan also suggests that this algorithm can be adapted for other collective sports. The article \cite{stephan} goes on to describe that the ``basketballVideoAnalysis'' project can track the match by executing a machine learning and image processing pipeline. This pipeline consists of basketball court detection, tracking humans inside the court, and mapping their movements to their position relative to the court.


Court detection is critical in this system because there is no reference needed to position objects (human bodies and ball) and filter out the noise (all remaining objects outside the court). The surface detection, in this article, is a pure image processing algorithm composed of naive OpenCV \cite{opencv} functionalities such as HSV conversion, Hue range filtering, Bitwise-AND masking, Canny edge detection and Hough Transformation. These functionalities' combination filters the input's noise out and increases the machine learning features' intensity.


The following step is to detect the people in the court, which is essential because it extracts the segmented image containing the person's to be used for further measures such as player identification, player tracking, homographic mapping, pose estimation and action recognition. The people detection is undertaken using robust common object detection models and training them against existing image curated datasets, for instance, the COCO image dataset \cite{cocodataset}.


The article points out how identifying players can be challenging by quoting an excerpt from ``Learning to Track and Identify Players from Broadcast Sports Videos'' \cite{learn_track_id} that describes the technical and logistical obstacles associated with this task.


The last step in Stephan's Project is to map the current camera view to a reference map to accurately determine where objects (including people) are in the field. In this operation, the OpenCV \cite{opencv} ``findHomography'' takes the original image points and the reference map coordinates to transform and map the original image onto the reference map.


Despite this article being a helpful starting point for research, it lacks academic research and implementation rigour. This is evident by the absence of any meaningful citation.


\section{Player tracking and analysis of basketball plays}

This project \cite{baskettrack} focuses on creating automated player detection and tracking system using the Matlab programming language. The basketball court detection in this Project is accomplished solely by using image processing techniques such as converting the colour format, eroding and dilating the image before applying the canny edge detector, and Hough transform. This process of court detecting appears to be very efficient as it is not a complex algorithm. This is highly effective for basketball, but it might not work as well in a football match. A football pitch is bigger than in basketball, and the camera is zoomed in to focus on a particular pitch zone. This fact, in football, forces a possible solution to find a way of augmenting the possible location the camera is focused on. The same pipeline, created by ``Player tracking and analysis of basketball plays'' research, should be used in our Project, but it needs to be complemented by machine learning and augmentation because basketball recordings have a significantly bigger percentage on the court \cite{baskettrack}.


The player detection is applied inside the court detection region by executing the OpenCV's \cite{opencv} HOG detector (histogram of oriented gradients) to create bounding boxes around players. This detector focuses on measuring the gradients' orientation on image's regions to detect patterns that recognise human bodies. This technique needs to be adjusted, during the testing stage, by adjusting the detection threshold adjusted to filter out false positives or just general noise. The approach is generally very effective unless two or more players are crossing in the camera view field. The authors decided to run the OpenCV's BGR detector inside the HOG detector bounding box since teams usually have different colours. This player detection algorithm is simple and accomplishes most of its purposes; however, it does not address players from the same team crossing the camera view. This event is very common in collective sports and should be a high requirement due to its occurrence frequency. An algorithm that uses human pose posture detection or other machine learning algorithms should effectively mitigate this important issue.


The team, following the implementation of the player detection algorithm, moved to work on player tracking. After some development, they identified four common tracking scenarios:
\begin{enumerate}
    \item Player detected in consecutive frames: player was detected by HOG and BGR detectors;
    \item Neighbourhood estimate: player was only detected by the BGR detector in the neighbourhood of the previous HOG detection;
    \item Players added to frame;
    \item Players removed from the frame.
\end{enumerate}
The first two scenarios are relatively easy to handle by applying the minimum distance correlator that calculates the most likely last position according to the current position. The last two cases are more complicated than the previous ones, and the Project did not mitigate them. Some of the reasons were the tracking of false positives (players on the bench just next to the court) and the camera jitter, which can affect the court view and the players' position on the camera view itself. The project \cite{baskettrack} was not successful in this step perhaps because it did not try to create an image recognition system that would train to learn the players' visual features (including the shirt number), as well as creating a machine-learning algorithm to augment the possible movement of players off the screen considering the last and first (when it reappears) position/speed.


Finally, the tracking data needs to be mapped to a model court by running a homography matrix. This matrix is generated through an affine transformation relative to the camera view towards the basketball court. This process is effective because the court and players' position is already known, and the matrix operation is very efficient in a numerical computing programming language like Matlab.



\section{Learning to track and identify players from broadcast sports videos}


The research analyses how to pre-process video and detect, identify and track players from a camera recording and map them to a referential map. It is a very extensive explanation that introduces video processing and player identification. Despite having many relevant requirements, the researchers decided not to have action recognition \cite{learn_track_id}.

Video pre-processing is composed of two steps: video segmentation and then ``play-by-play'' data processing. This pre-processing combination captures and aggregates relevant data used for data augmentation on the next pipeline layers. The video segmentation takes full recordings (sequence of different camera/video shots such as replays, ads, focus zoom), trains a complex model that uses Hidden Markov Model, Gaussian Mixture Model and the Viterbi algorithm to predict the segments and which type they are based on the colour distribution. The ``play-to-play'' data is a sequential, freely available online stream that tracks the most important match events. This type f data is very important in the player identification and could be very useful for action recognition if it was a requirement \cite{learn_track_id}.

The player tracking detects the players inside the court by running a Deformable Part Model, which can identify most players, but it has mediocre precision since it detects people in the audience and the referees, and it does not detect players obstructed in the camera angle. Before training a Linear Logistic Classifier, the false positives (audience and referee) are removed by applying team colour (since all teammates have the same colour but are different from the opposition or referee). The player tracking follows the bounding boxes' centre points from the player detections and tries to predict the following position based on the Euclidean distances of previous positions by executing a linear-Gaussian transition model. The precision of this tracking technique has a very high precision since it only tracks single points (simple and minimal algorithm) and drops sparse tracking or view borders signals \cite{learn_track_id}.
The player identification is an exciting and unique challenge set by the researchers. It poses serious questions about how its efficiency due to:

\begin{itemize}
    \item same team players have the same kits;
    \item recording with resolution too low for facial recognition;
    \item some players might have similar body characteristics, hairstyles, or boots;
    \item shirt numbers and names are deformed due to constant posture changes \cite{learn_track_id};
\end{itemize}


The solution for player identification focuses on learning players' visual features/characteristics, dismissing trying to identify players' faces \cite{learn_track_id}. The input to identify these features are:

\begin{itemize}
    \item Maximal extremal regions (MSRE);
    \item Scale-invariant feature transform (SIFT);
    \item RGB colour histograms;
\end{itemize}


These image processing features and the bounding box image segment are the input for a complex algorithm that continuously learns each player's visual characteristics and identifies them \cite{learn_track_id}.
The homography transformation is calculated by representing the referential basketball court as a set of points. The Canny Edge detector processes the image with a certain threshold to remove noise and drop nonlinear or well-rounded lines until only the court shape is visible. Following this process constructs the homography matrix by incrementally detecting the corresponding points (nearest points) until it converges in at most five iterations, at most \cite{learn_track_id}.

\section{Multi-Person 3D Pose Estimation and Tracking in Sports}






This Project aims to process multiview recordings, detect and correct players' 2D pose estimations, associate the 2D poses into a 3D pose and track the 3D poses. Pose estimation is very important because it has the potential to allow for action recognition. The researchers consider this Project as the pioneering in full 3D pose estimation tracking for sports. The Project used advanced research from previous work on 3D pose estimation and multiview video recording to develop a very optimised process to capture this kind of data for Football \cite{cvpr_2019}. The algorithm follows these general steps:

\begin{itemize}
    \item 2D pose estimation
    \item 2D pose error correction
    \item 2D pose multiview association to 3D pose
    \item 3D skeleton tracking
\end{itemize}

The Project uses a CNN-based model to train in the ``Surreal'' synthetic dataset \cite{surreal} and subsequently estimate the 2D pose of players.
The initial pose estimation is prone to errors due to low-quality image, view cut, and multi-person fusion, so the pose error correction is vital to correct these common errors. The researchers decided on a per-frame pose correction solution rather than a temporal filtering and tracking algorithm because its performance is significantly better \cite{cvpr_2019}.
The next step is to associate the two 2D pose estimations from a single frame by running a greedy algorithm that measures the pair of corresponding points. Once this correspondence is asserted, then the 3D joint locations can be calculated. Once the 3D joint location is calculated, then it can be constructed and mapped to the 3D court model \cite{cvpr_2019}.
The final step is to sequentially track the resultant 3D pose models around the pitch what is very impressive because it is a full 3D model of a football match \cite{cvpr_2019}.






\section{Soccer: Who Has The Ball? Generating Visual Analytics and Player Statistics}


This Project is a solution that generates visual analytics and player statistics from football recordings. This is an important paper because it aims to recognise actions using deep neural networks from single frames in football matches. The paper also describes the architecture of the different deep neural networks used for, as mentioned before, for the action recognition but also the player location detection \cite{cvpr_2018}.




The first part of this algorithm is to locate, track the player and detect the team. The location is done by running the YOLO algorithm, which predicts the bounding boxes containing every player on the camera view. These bounding boxes are then tracked during the consecutive by another model also using the YOLO algorithm. This tracking phase is then finished by running a Histogram Matching algorithm inside the bounding box to identify the player's team. The Histogram Matching algorithm was chosen because it is fast at detecting different colours (teams' kits are always different), and the task is relatively simple since the player is inside a small region \cite{cvpr_2018}.






The last step is to identify if the player is ``in control of the ball'' or not and then augment data from this information. The researchers created an image dataset with images (segments of images, zoomed into the player) of individual football players and then classified the images with either ``player with the ball'' or ``player without the ball'' classes. This part of the algorithm then took the synchronised sequence of images and predicted data to keep track of the player holding the ball. This information is relevant because it is possible to deduce with spatio-temporal precision the possession of the ball by a team since it is the sequence of players ``having the ball''. The spatio-temporal event between players of the same team ``having the ball'' is a pass (where the start and end location is also known) otherwise is an opposition ``ball recovery''. Generating an automatic model of possession is precious because it is the most frequent activity the football teams engages in, and it is the most relevant way football domain experts take into consideration when analysing football \cite{cvpr_2018}.




\section{Summary}




This background research was vital to understanding the foundations of the current solutions in the field. Based on the current acquired information, this Project's goal is possibly accomplished by decomposing the problem into a pipeline composed by:

\begin{itemize}
    \item Court detection;
    \item Object detection;
    \item Team identification;
    \item Object tracking;
    \item Player identification;
    \item Homographic mapping of objects.
\end{itemize}


The important idea to remember is that there is already robust technologies and methodologies necessary for this solution. This knowledge gained should provide a solid foundation to develop a proper implementation of a similar solution.



\chapter{Implementation}







To add to methods section




pseudocode for main code





\begin{algorithm}[H]
\begin{algorithmic}
\caption{findObjects procedure}\label{alg:findObjs}
\Require \\
    $frame: OpenCVFrame$ \\
    $model: YoloModel$ \\
    $c: ObjectConfidenceThreshold$ \\
    $b: BallConfidenceThreshold$
\Ensure \\
    $objs: ObjectsBoxes$ \\
    $ball: BallBox$
\State
\State $predictions \gets model.predict(frame)$
\State $objs \gets \emptyset$
\For {$prediction \in predictions$}
    \If {$prediction \Is person$ \And $c < conf$}
        \State $objs.append(prediction)$
    \EndIf
    \If {$prediction \Is ball$ \And $b < conf$}
        \State $ball \gets prediction$
    \EndIf
\EndFor
\State
\Return $objs, ball$
\end{algorithmic}
\end{algorithm}


\ref{alg:findObjs} text


\begin{algorithm}[H]
\begin{algorithmic}
\caption{Object detection and tracking procedure}\label{alg:detect_tracking}
\State $frameCount \gets 0$
\For { $frame \in video$ }
    \If { $frameCount \bmod 30 = 0$ }
        \State $objs, ball \gets findObjects(frame)$
        \If {$length(objs) > 0$ }
            \For { $o \in objs$ }
                \State $objTracker.add(o)$
            \EndFor
            \State $frameCount \gets frameCount + 1$
        \Else
            \State $frameCount \gets 0$
        \EndIf
        \If { $ball$ }
            \State $ballTracker.init(ball)$
        \Else
            \State $ball \gets labelBall(frame)$
            \State $ballTracker \gets Tracker(ball)$
        \EndIf
    \Else
        \State $isTracking, boxes \gets objTracker.update(frame)$
        \If { $isTracking$ }
            \State $frameCount \gets frameCount + 1$
        \Else
            \State $frameCount \gets 0$
        \EndIf
        \State $isTracking, box \gets ballTracker.update(frame)$
        \If {$\Not isTracking$}
            \State $ball \gets labelBall(frame)$
            \State $ballTracker \gets Tracker(ball)$
        \EndIf
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}


\ref{alg:detect_tracking} text


\item
  pseudocode for 3-d modelling data set generation
\end{itemize}


\begin{algorithm}[H]
\begin{algorithmic}
\caption{encoding helper procedures}\label{alg:enc_help}
\Procedure{encode\_vector}{$vector$}
    \Return $tuple($
        \State \indent $vector.x, vector.y, vector.z$
    \State $)$
\EndProcedure
\Procedure{encode\_vector\_list}{$list$}
    \Return $reduce($
        \State \indent $\lambda rest\_data, last: rest\_data + encode\_vector(last),$
        \State \indent $list,$
        \State \indent \emptyset \Comment{$initial value$}
    \State $)$
\EndProcedure
\Procedure{corner\_vecs\_pitch\_vecs}{$pitch\_vectors$}
    \State $corner\_vectors \gets \emptyset$
    \State $p\_corners \gets \emptyset$
    \For {$i \in pitch\_vectors$}
        \If {$i \in \{1, 10, 30, 39\}$}
            \State $corner\_vectors.append(pitch\_vectors[i])$
        \Else
            \State $p\_corners.append(pitch\_vectors[i])$
        \EndIf
    \EndFor
    \State
    \Return $($
        \State \indent $corner\_vectors,$
        \State \indent $p\_corners$
    \State $)$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\ref{alg:enc_help} text


\begin{algorithm}[H]
\begin{algorithmic}
\caption{encodeData procedure}\label{alg:enc_data}
\Procedure{encodeData}{$data$}
    \State $origin, framesVectors, pitchVectors \gets data$
    \State $encodedOrigin \gets encode\_vector(origin)$
    \State $encodedFramesVectors \gets encode\_vector\_list(framesVectors)$
    \State $corner\_vectors, pitch\_vectors \gets corner\_vecs\_pitch\_vecs(pitch\_vectors)$
    \State $encodedCornerVectors \gets encode\_vector\_list(pitchVectors)$
    \State $encodedPitchVectors \gets encode\_vector\_list(pitchVectors)$
    \State
    \Return $($
        \State \indent $encodedOrigin +$
        \State \indent $encodedFramesVectors +$
        \State \indent $encodedCornerVectors +$
        \State \indent $encodedPitchVectors$
    \State $)$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\ref{alg:enc_data} text


\begin{algorithm}[H]
\begin{algorithmic}
\caption{Dataset generation procedure}\label{alg:data_gen}
\Procedure{getData}{$camera$}
    \State $camOriginVector \gets camera.matrix.translation()$
    \State $framesVectors \gets camOrigin.frames()$
    \State $pitchVectors \gets \emptyset$
    \For {$marker \in blender.collection(``pitch markers'')$}
        \State $pitchVectors.append(marker)$
    \EndFor
    \State
    \Return $origin, framesVectors, pitchVectors$
\EndProcedure
\For {$camera \in cameras$}
    \State $fileName \gets blender.renderImage(camera)$
    \State $data \gets getData(camera)$
    \State $encodedData \gets encodeData(data)$
    \State $writeToCsv(fileName, encodedData)$
    \State $camera.changeAngle()$
\EndFor
\end{algorithmic}
\end{algorithm}


\ref{alg:data_gen} text


\begin{itemize}
\item
  machine learning model and algorithm
\end{itemize}


\begin{algorithm}[H]
\begin{algorithmic}
\caption{Model compiling procedure}\label{alg:model_comp}
\Require \\
    data: ModelData \\
    params: ModelParameters \\
    w: IMG\_WIDTH \\
    h: IMG\_HEIGHT
\State
\State $outputSize \gets params.outputSize$
\State $secondaryInputLen \gets params.secondaryInputLen$
\State $convolutionLayers \gets [Input(w, h), Convolution2D(), flatten()]$
\State $output \gets Output(outputSize)$
\If {$secondaryInputLen > 0$ }
    \State $secondaryInput \gets [Input(secondaryInputLen)]$
    \State $model \gets Model($
        \State \indent $input \gets concatenate(secondaryInput, convolutionLayers),$
            \State \indent $outputs \gets output$
    \State $)$
\Else
    \State $model \gets Model($
        \State \indent $input \gets convolutionLayers,$
        \State \indent $outputs \gets output$
    \State $)$
\EndIf
\State $compileModel(model, data, params)$
\end{algorithmic}
\end{algorithm}


\ref{alg:model_comp} text



\begin{algorithm}[H]
\begin{algorithmic}
\caption{Models training procedure}\label{alg:model_train}
\Require \\
    data: ModelData \\
    params: ModelParameters
\State
\State $trainModel( data, ($
    \State \indent $model \gets ``camOriginVec'', outputSize \gets 3, secondaryInputLen \gets 0,$
    \State \indent $\ldots params$ \Comment{$rest$}
\State $) )$
\State $trainModel( data, ($
    \State \indent $model \gets ``frameVectors'', outputSize \gets 12, secondaryInputLen \gets 3,$
    \State \indent $\ldots params$ \Comment{$rest$}
\State $) )$
\State $trainModel( data, ($
    \State \indent $model \gets ``pitchCornerVecs'', outputSize \gets 8, secondaryInputLen \gets 15,$
    \State \indent $\ldots params$ \Comment{$rest$}
\State $) )$
\State $trainModel( data, ($
    \State \indent $model \gets ``pitchVectors'', outputSize \gets 70, secondaryInputLen \gets 23,$
    \State \indent $\ldots params$ \Comment{$rest$}
\State $) )$
\end{algorithmic}
\end{algorithm}


\ref{alg:model_train} text


\begin{algorithm}[H]
\begin{algorithmic}
\caption{Recursive model prediction}\label{alg:rec_pred}
\Procedure{getModelPred}{$modelNames, X$}
    \If {$modelNames = \emptyset $}
        \State
        \Return $X$
    \EndIf
    \State $curr\_model \gets modelNames[0]$
    \State $rest\_models \gets modelNames[1:]$
    \State $pred \gets curr\_model.predict(frame, X)$
    \State
    \Return $getModelPred(rest\_models, X + pred)$
\EndProcedure
\Procedure{getFramePrediction}{$frame$}
    \State
    \Return $getModelPred($
        \State \indent $[``camOriginVectors'', ``frameVectors'', ``pitchCornerVectors'', ``pitchVectors''],$
        \State \indent \emptyset
        \State
    \State $)$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\ref{alg:rec_pred} text


\begin{itemize}
\item
  geometry reconstruction algorithm
\end{itemize}


\begin{algorithm}[H]
\begin{algorithmic}
\caption{Decoding helper procedures}\label{alg:dec_help}
\Procedure{dec\_vec}{$list$}
    \Return $tuple(list[0], list[1], list[2])$
\EndProcedure
\Procedure{dec\_list}{$data, decode\_fun, unit\_length$}
    \State
    $data\_len \gets length(data)$
    \If {$data\_len = 0$}
        \Return $data$
    \EndIf
    \State $rest\_of\_data \gets data[:data\_len - unit\_length]$
    \State $data\_to\_decode \gets data[data\_len - unit\_length:]$
    \State $decoded \gets decode\_fun(data\_to\_decode)$
    \State
    \Return $dec\_list($
            \State \indent $rest\_of\_data,decode\_fun,unit\_length$
    \State $) + [decoded]$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\ref{alg:dec_help} text


\begin{algorithm}[H]
\begin{algorithmic}
\caption{Decoding camera data}\label{alg:dec_data}
\Procedure{decode\_vector\_list}{$list, length$}
    \Return $dec\_list($
        \State \indent $list,$
        \State \indent $\lambda data: dec\_vec(data, length),$
        \State \indent $length$
    \State $)$
\EndProcedure
\Procedure{decode\_camera\_data}{$encoded\_data$}
    \State $($
    \State \indent $origin\_data, frames\_vectors\_data, corner\_vectors\_data, pitch\_vectors\_data$
    \State $) \gets get\_data\_ranges(encoded\_data)$
    \State $origin\_decoded \gets dec\_vec(origin\_data, 3)$
    \State $frames\_vectors\_decoded \gets decode\_vector\_list(frames\_vectors\_data, 3)$
    \State $corner\_vectors\_decoded \gets decode\_vector\_list(corner\_vectors\_data, 3)$
    \State $pitch\_vectors\_decoded \gets decode\_vector\_list(pitch\_vectors\_data, 3)$
    \State $pitch\_vectors\_decoded \gets merge($
        \State \indent $corner\_vectors\_decoded,$
        \State \indent $pitch\_vectors\_decoded$
    \State $)$
    \State
    \Return $tuple(origin\_decoded, frames\_vectors\_decoded, pitch\_vectors\_decoded)$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\ref{alg:dec_data} text


\section{Not yet fully implemented}

- not integrated the geometric reconstruction


\begin{itemize}
\item
creating geometric framework to map objects from screen on pitch, and from pitch on to file
\end{itemize}

\begin{itemize}
\item
  pitch geometry reconstruction

  \begin{itemize}
  \item
    optical distortion
  \item
    algorithm to determine inner section
  \item
    image homographical transformation
  \item
    grid positioning
  \item
    map players and ball to pitch
  \end{itemize}
\end{itemize}


\begin{itemize}
\item
    geometry reconstruction
    \begin{itemize}
      \item
        determine objs position
      \item
        homographical transformation ???
    \end{itemize}
\end{itemize}





\section{3d modelling and dataset generation}


text



\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{coordinates.png}{text
    \caption{Reference Map}
    \label{img:ref_map}
\end{figure}
\ref{img:ref_map} description} created 3d reference system that maps the points recognisable by the camera. This will be used for the artificial intelligence model to train the model and to process the video stream to perform the homographic transformation.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_from_2021-10-22_13-59-52.png}
    \caption{Development of 3D blender model}
    \label{img:texture}
\end{figure}
\ref{img:texture} The 3d model was developed on blender. It is a green 3d texture (to emulate the grass) with a pitch png transparent graphic to produce the white lines.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2021-12-17_19-08-06.png}
    \caption{Camera view in blender}
    \label{img:cam_grass}
\end{figure}
\ref{img:cam_grass} The result is a realistic pitch replica that can be rendered by a blender camera to produce the synthetic dataset

\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{image.png}
    \caption{Camera rendered image}
    \label{img:camera_view}
\end{figure}
\ref{img:camera_view} this is the rendered image from a blender camera, this image is then the processed to be then used as the input for the artifical intelligence model.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-05_12-03-42.png}
    \caption{Preview of the finished blender model}
    \label{img:blender_preview}
\end{figure}
\ref{img:blender_preview} The pitch was then improved by adding 3d markers corresponding to the map reference system. These markers can be accessed by the blender cameras to retrieve their position on the camera view and their relative position to the cartesian origin.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-05_12-45-59.png}
    \caption{Penalty box view}
    \label{img:penalty_box}
\end{figure}
\ref{img:penalty_box} This is the view of the box which shows the position of the markers from a closer view
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-05_12-47-42.png}
    \caption{Goal}
    \label{img:goal}
\end{figure}
\ref{img:goal} This is the view from the goal which is an important object that is important for image recognition. it has the only markers with a positive z-index to emulate the top corners of the goal. It also includes the corner flag (on the left) because all pitches have them by regulation.
\begin{figure}[H]
    \includegraphics[keepaspectratio, width=\columnwidth]{Screenshot_2022-03-05_12-05-23.png}
    \caption{Cameras}
    \label{img:cameras}
\end{figure}
\ref{img:cameras} The final step is to create 15 cameras which will rotate within a range and will render images for the dataset from these different position to emulate the real camera which will be put in different positions.





\chapter{Testing}



\section{Testing assessment}

\begin{itemize}
\item
  testing with random camera, get accuracy
\item
  testing machine learning model accuracity over many layers

  \begin{itemize}
  \item
    configuration of convolution

    \begin{itemize}
    \item
      their kernels sizes
    \item
      number of filters
    \end{itemize}
  \item
    size of pool layers
  \item
    dropout rate
  \item
    number of flat dense layer when it has 2 inputs
  \end{itemize}
\end{itemize}



\section{Performance assessment}


text



\chapter{Conclusion}



\section{Main achievements}

    (relating them to initial objectives)
    (as well as similar worh from others)



\section{Main limitations of work}


This project accomplished most of its goals but there are many aspects that are, at least for the moment, not achievable. The main and ultimate limitation is that the output will always be an approximation/estimation of the real-world since it captures data from the analog world to the digital one. Recognising this irrefutable limitation, gives the correct perspective on what is possible and to seek to improve or create. Future development must have this assumptions always present.


The data comes only from the camera view. This fact underlies a relevant limitation. It is impossible to know exactly where players off the camera view are. The position off all players at all times is valuable in football data science projects. The promising solution is to mitigate the issue by developing a spatio-temporal model of players' positions on previous matches. This could be used to give an estimate of where the missing players could be.


Identifying players is essential in sports data science as there is a need to calculate the performance of each individual player. At the moment this solution does not do visual recognition of players as it goes beyond of the current goals. It will, however, a priority goal once the solution's basic functionality is completed.


The ball and players will, in the initial stage, be mapped on a 2D dimensions. This does not reflect the nature of the sport. The players and the ball have movements on the 3D space (humans jumping and ball being shot) and it is very important data, especially the ball trajectory in shots in order to analyse expected goals metric. Additionally, the current ball 2D trajectory tracking is also prone to be interrupted whenever a object obstructs the camera view. This is a major limitation that needs to be addressed as a priority for this solution to be viable. These issues are complex in their nature and require special planning to find the most effective approach.




\section{Future work}




The solution has great potential so there is a long list of future work to fulfil it. The good improvement is to parallelise and multithread the current program which was not done yet because it is still in an experimental phase and it is not time efficient to invest in applying these techniques on code that may be changed or removed. The performance improvement will be implemented once the program reaches a good level of efficacy. Additionally, the execution, training and testing of the many modules of the solution has been done solely on a local machine that is not able to run as fast as a system like this should. For this reason, the code should be moved to a cloud platform so that it increases the performance drastically.


The more general improvements that could add immediate value is to improve the image processing and the detection/tracking system to work with low quality recordings resultant from bad weather conditions or bad lighting conditions. Besides that, the system uses a bitwise "and" to apply a green mask to the video but not all pitches are green. This could be solved by letting the operator choose the "masking colour" for pitches that do not have green grass or if the pitch is snowy.


An essential component of sports data science is to evaluate the performance of each individual and that is only possible if each player is identified when the data is being collected. The identification could first start by using the numbers on shirts to identify players so that an additional model could then learn the visual characteristics (body visuals and boots) and then predict the players when the numbers are not visible by their visuals. In this case, the referees should be filtered by their kit colour because the team of officials has always a different colour.


In the first runs of the human detection algorithm a bug emerged that detected a huge sized human detection. This is clearly a bug but it was not fixed because a neat algorithm was not found to this moment that could filter it in a consistent matter. Different recordings will have produce different sized bounding boxes so it is not wise to choose a fixed threshold to filter. The solution should calculate the median and standard deviation of all human detections before filtering the outcasts.


Any team sport is dynamic meaning that a match has many plays (defined as segments of match, segmented by set pieces) and the video stream have regularly replays or sudden angles changes. These events should be processed by the system. These segments should be dealt with firstly manually (following the development strategy) by giving the operator control of segmentation. The final solutions should be a unsupervised clustering machine learning model that processes a stream of images to detect these segmentations.


Data generation has significant problem that happens if the camera frame points beyond the horizon in the 3d model. This is important because smaller stadium have lower stands so camera angles will be pointing higher than in bigger stadiums. It is necessary to return a inner segment to detect do the geometric reconstruction. In this issue, the solution may be approximating somehow the inner section to a sensible and workable value. The dataset generation could also improve by adding noise to the images in the dataset to emulate a real-world scenario.


The solution's data collection is expected to be chaotic due to the intermittent nature of object detection and tracking of objects and geometric distortion. This is a serious issue that can only be solved by creating a spatio-temporal data stream correction to smooth and generally fix inconsistencies in the data output.


The nature of collective sports is set of relationships between individual players. The sport, as function, can be reasoned based on its spatio-temporal states. This definition of sports can entail that a logical programming language and/or logical reasoner could be useful if applied to it. As such, outputting the data as an ontology could be really promising for sports data science.


Human pose estimation is a very interesting technique that could be very insightful to analyse the body performance and language of players. It would enrich the data to give much more context to the analysis, especially if it could be used to train an action predicting model. There is already a few dataset of past matches containing a match's event stream (passes, shots, tackles, etc.) and that could be used to undertake extrapolation and use it to predict many kind of types of actions with really good accuracy, at least in theory.


The greatest future improvement could be the integration of a 3d simulation environment which can be used to firstly enhance the current capabilities and additionally add extra functions that could take this project to the next level as a another source of synthetic data which could be complementary to the existing blender generation dataset environment. The Google Research Football Environment \cite{gfootball_env}, is a former open-source football computer game that was repurposed by Google's Artificial intelligence department to test and train AI adversary models in a highly stochastic environment. The project has been successful and it was used in many different AI events and competitions. It can be controlled by humans but the ultimate goal is to each team/players to be controlled by an intelligent agent in a realistic 3d physical environment that generates 3d positional data of all objects in the pitch. After training these models, the teams play against each other and generate important data that is used to evaluate the models performance. This final output data, along the corresponding visual data recorded, could be used to improve the training/validation/testing of the existing machine learning models for object and geometric detection models since the data collected intersects the one generated on blender. Although promising, the real value is to use the in-game simulator to extract data that signals actions undertaken by players, 3d player pose estimation, objects 3d trajectory, in-game play segments and set piece recognition. These additional features are very valuable to increase the data points and enrich the data which contributes to automate ever more some manual operation that are performed by the operator. It is also conceivable to create an extra model that corrects players tracking (to smoothing it out), and even augmenting the off-screen player positions (by combining spatio-temporal in-game data) which is truly groundbreaking. Predicting the off-screen players position is critical because it mitigates one of the biggest limitations of this approach of using regular match-streams.


The Google Fooball environment \cite{gfootball_env} is specific to football what is contrary to objective of being a general solution for any sports. The long term solution in to create free- and open-source modular game environment to simulate other sports and even be used as bases for standalone sports games. This proposal is far fetched as it is a whole another project by itself but is is a sustainable project to leverage sports data science and gaming development. For these reasons this could be added to the community's long term roadmap.



\printbibliography


\listoffigures


\listofalgorithms


\end{document}
